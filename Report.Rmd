---
title: "Evaluating the Impact of Baseline Hazard Function Misspecification on Treatment Effect Estimation"
author: "Charly Fowler, Hanfei Qi, Robert Tumasian III , and Haoyang Yi"
date: "February 24, 2021"
output: pdf_document
---

```{r, include=FALSE}
library(tidyverse)
library(png)
```

# Objectives

The goal of this study is to evaluate how misspecifying the baseline hazard function can influence the estimation of treatment effects in survival without censored observations (complete data available for all subjects; no missingness due to dropout or event occurrence). This work focuses on conducting simulations to compare the exponential and Weibull proportional hazards models to the Cox proportional hazards model. We also discuss the impact of utilizing an overly complicated model (e.g., Cox) when a less complex model (e.g., exponential) is sufficient.

# Statistical Methods

Survival analysis is used to analyze time-to-event data (e.g., time to symptom onset or time to mortality). Survival functions, $S(t)$, measure the probability of an individual not experiencing an event beyond a certain time $t$. Similarly, hazard functions, $h(t)$, measure the instantaneous risk of failure at a certain time $t$, given that the individual has not experienced an event until that time. The hazard function can be expressed as $\frac{f(t)}{S(t)}$, where $f(t)$ is the distribution of survival times.\par

One purpose of proportional hazards modeling is to assess the effectiveness of a particular treatment ($X$) over survival time $T$, where the hazard ratio for patient $i$ at time $t$ is defined as $h_i(t)=h_0(t)e^{x_i\beta}$. Here, $h_0(t)$ denotes the pre-specified baseline hazard function, $x_i$ indicates treatment allocation (0=control, 1=treatment), and $\beta$ represents the log hazard ratio, or the hazard reduction among treated individuals compared to the control group. Thus, the proportional hazard can be expressed as $\frac{h(t|x_0)}{h(t|x_1)}=e^{\beta(x_0-x_1)}$, which is independent of survival time $t$.\par

# Simulation Design

We consider three proportional hazards models (exponential, Weibull, and Cox) and six baseline hazard functions (exponential [$\gamma=1$], Weibull [$\gamma=0.5,2,5$], lognormal, and piecewise). Note that the exponential and Weibull models are equivalent when $\gamma = 1$. All continuous baseline hazard functions are provided in Table 1 below.\par

\newpage

$$
\begin{array} {lccc}
\hline \text {} & \text { Exponential } & \text { Weibull } & \text { Lognormal } \\
\hline
\text { $h_0(t)$ } & \lambda & \lambda\gamma t^{\gamma-1} & \frac{\frac{1}{t \sigma} \phi(\frac{ln(t)}{\sigma})}{\Phi(\frac{-ln(t)}{\sigma})} \\
\hline
\end{array}
$$

Table 1. Continuous baseline hazard functions, $h_0(t)$, considered in this study. The normal PDF and CDF are denoted by $\phi$ and $\Phi$, respectively.

\bigskip

The exponential and Weibull models can implement different baseline hazard functions, while the Cox model estimates $\beta$ without this specification. It is important to mention that even if the baseline hazard function is known, the Cox model is still expected to perform well due to its semi-parametric efficiency (Anderson et al, 1982). All three models impose the restraint that the effect of the treatment be multiplicative on the hazard curve. The baseline hazard curves we consider are shown in Figure 1 below.\par

\bigskip

```{r, echo= F, warning = F}
#plot hazards: 

set.seed(1729)
ncuts <- 19
cuts <- sort(rexp(ncuts, rate = 0.1))
pw_times <- c(0, cuts)
N <- length(pw_times)
pw_haz <- sort(abs(rnorm(N)))
pw_haz <- abs(pw_haz - median(pw_haz))

haz_piece <- function(t, ...) {
   pw_haz[findInterval(t, pw_times)]
}

haz_log <- function(t) {
  ((1/t*0.5)*dnorm(log(t)/0.5,mean=-0,sd=0.5))/
    (pnorm(-log(t)/0.5,mean=0,sd=0.5))
}

haz_weibull_0.5 = function(t){
  0.5 * t^(0.5-1)
}

haz_exponential = function(t){
  1 * t^(1-1)
}

p <- ggplot(data = data.frame(x = 0), mapping = aes(x = x))

p + 
  stat_function(fun = haz_piece, aes(color = "piece-wise")) + 
  stat_function(fun = haz_log, aes(col = "log-normal")) + 
  stat_function(fun = haz_weibull_0.5, aes(col = "weibull (gamma = 0.5)")) + 
  stat_function(fun = haz_exponential, aes(col = "pink")) + 
  xlim(0,20) + 
  ylim(0,2) + 
  theme_minimal() + 
  scale_color_manual(name = "Hazard Functions", 
                     values = c("red", "green", "blue", "orange"), 
                     labels = c("Lognormal", "Piecewise", "Exponential", "Weibull (gamma = 0.5)" )) + 
  theme(legend.position="bottom") + 
  labs(x = "t", y = "Hazard")
  
```

Figure 1: Baseline hazard functions used in this study for data generation. The lognormal distribution uses $\mu=0$ and $\sigma=0.5$. The piecewise distribution (CHARLY: ADD HOW WAS THIS MADE). The Weibull distributions with $\gamma = 2,5$ have been omitted due to scaling (very steep growth).

\bigskip

All simulation data was generated using the `simsurv` function in the `simsurv` package. We defined a binomial treatment variable (`trt`) with $p=0.5$ to ensure equal likelihood of random assignment to the treatment or control group. The resulting dataset contains time of event (`eventtime`), status (`status`), and treatment group (`trt`). Since we did not simulate censored observations, the `eventtime` variable represents time of event for all subjects.\par

We simulated survival data from six different baseline hazard distributions, using $\beta = -0.5$ as the true treatment effect size. First, we simulated from an exponential distribution with $\lambda = 1$ and $\gamma=1$, and then from a Weibull distribution with $\lambda = 0.1$ and $\gamma=0.5,2,5$. These values of $\gamma$ were chosen to consider monotone decreasing ($\gamma=0.5$), constant ($\gamma=1$), and monotone increasing ($\gamma=2,5$) baseline hazard curves. Next, we simulated from a lognormal ($\mu=0,\sigma=0.5$) and piecewise distribution (inspired by Brilleman et al.) to consider non-monotone baseline hazard functions and to better understand the implications of misspecifying the underlying distribution of survival times when fitting a model.\par

Altogether, 1000 survival datasets containing 100 samples were simulated for each baseline hazard function. Each dataset was used to fit all three proportional hazards models and extract the estimated treatment effects ($\beta$).

# Results
To assess model performance, we plotted the survival probability versus time of all models with a sample from each simulation scheme. We also used mean-squared error (MSE) to check model performance. 

\begin{figure} [H]
{
\includegraphics[]{Gamma_0.5.png}
}
\end{figure}

```{r suv vs time (0.5), echo=FALSE,include=FALSE}
pp <- readPNG("Gamma_0.5.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```

Figure 2. Survival probability versus time of each model, baseline hazard function follows weibull distribution with $\gamma = 0.5$.

```{r suv vs time (1), echo=FALSE,include=FALSE}
pp <- readPNG("Gamma_1.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```

Figure 3. Survival probability versus time of each model, baseline hazard function follows Exponential distribution.

```{r suv vs time (2), echo=FALSE,include=FALSE}
pp <- readPNG("Gamma_2.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```

Figure 4. Survival probability versus time of each model, baseline hazard function follows weibull distribution with $\gamma = 2$.

```{r suv vs time (5), echo=FALSE,include=FALSE}
pp <- readPNG("Gamma_5.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```

Figure 5. Survival probability versus time of each model, baseline hazard function follows weibull distribution with $\gamma = 5$.

```{r suv vs time logn, echo=FALSE,include=FALSE}
pp <- readPNG("lognormal.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```

Figure 6. Survival probability versus time of each model, baseline hazard function follows lognormal distribution.

```{r suv vs time pw, echo=FALSE,include=FALSE}
pp <- readPNG("piecewise.png")
plot.new() 
rasterImage(pp,0,0,1,1)
```

Figure 7. Survival probability versus time of each model, baseline hazard function follows piecewise distribution.
  
```{r, include=FALSE}
path_to_gamma0.5= "./results_gamma0.5.csv"
path_to_gamma1= "./results_gamma1.csv"
path_to_gamma2= "./results_gamma2.csv"
path_to_gamma5= "./results_gamma5.csv"
path_to_logn= "./results_lognormal.csv"
path_to_piecewise = "./results_piecewise.csv"
result_0.5 = read_csv(file = path_to_gamma0.5)
result_1 = read_csv(file = path_to_gamma1)
result_2 = read_csv(file = path_to_gamma2)
result_5 = read_csv(file = path_to_gamma5)
result_ln = read_csv(file = path_to_logn)
result_pw = read_csv(file = path_to_piecewise) # Load the sets

mean_beta = tibble(
  Model = c("Exponential", "Weibull", "Cox"),
  Gamma_0.5 = c(mean(result_0.5$exp_beta), mean(result_0.5$weibull_beta), mean(result_0.5$cox_beta)),
  Gamma_1 = c(mean(result_1$exp_beta), mean(result_1$weibull_beta), mean(result_1$cox_beta)),
  Gamma_2 = c(mean(result_2$exp_beta), mean(result_2$weibull_beta), mean(result_2$cox_beta)),
  Gamma_5 = c(mean(result_5$exp_beta), mean(result_5$weibull_beta), mean(result_5$cox_beta)),
  Lognormal = c(mean(result_ln$exp_beta), mean(result_ln$weibull_beta), mean(result_ln$cox_beta)),
  Piecewise = c(mean(result_pw$exp_beta), mean(result_pw$weibull_beta), mean(result_pw$cox_beta))
) %>%
  knitr::kable() # mean of 1000 beta
MSE = function(beta, n=1000) {
  return(sum((-0.5 - beta)^2)/n)
}
```

```{r, echo=FALSE}
MSE_table = tibble(
  Model = c("Exponential", "Weibull", "Cox"),
  Gamma_0.5 = c(MSE(result_0.5$exp_beta), MSE(result_0.5$weibull_beta), MSE(result_0.5$cox_beta)),
  Gamma_1 = c(MSE(result_1$exp_beta), MSE(result_1$weibull_beta), MSE(result_1$cox_beta)),
  Gamma_2 = c(MSE(result_2$exp_beta), MSE(result_2$weibull_beta), MSE(result_2$cox_beta)),
  Gamma_5 = c(MSE(result_5$exp_beta), MSE(result_5$weibull_beta), MSE(result_5$cox_beta)),
  Lognormal = c(MSE(result_ln$exp_beta), MSE(result_ln$weibull_beta), MSE(result_ln$cox_beta)),
  Piecewise = c(MSE(result_pw$exp_beta), MSE(result_pw$weibull_beta), MSE(result_pw$cox_beta))
) %>% # MSE of 1000 beta
  knitr::kable()
MSE_table
```




\begin{center}

\end{center}


Table 2. The MSE table of three models with each simulation data.

```{r, echo=FALSE}
Mean_table = tibble(
  Model = c("Exponential", "Weibull", "Cox"),
  Gamma_0.5 = c(mean(result_0.5$exp_beta), mean(result_0.5$weibull_beta), mean(result_0.5$cox_beta)),
  Gamma_1 = c(mean(result_1$exp_beta), mean(result_1$weibull_beta), mean(result_1$cox_beta)),
  Gamma_2 = c(mean(result_2$exp_beta), mean(result_2$weibull_beta), mean(result_2$cox_beta)),
  Gamma_5 = c(mean(result_5$exp_beta), mean(result_5$weibull_beta), mean(result_5$cox_beta)),
  Lognormal = c(mean(result_ln$exp_beta), mean(result_ln$weibull_beta), mean(result_ln$cox_beta)),
  Piecewise = c(mean(result_pw$exp_beta), mean(result_pw$weibull_beta), mean(result_pw$cox_beta))
) %>% # Mean of 1000 beta
  knitr::kable()
Mean_table
```


\begin{center}

\end{center}


Table 3. The Mean table of three models with each simulation data.


# Conclusions

By comparing the Mean square error(MSE) and mean of $\beta$ derived from three models in different simulations, we found that misspecifying the baseline hazard function can lead to significant differences between actual value of $\beta$ and the truth(which was set as -0.5). 

For example, we got high MSEs and biased means of $\beta$ when applying exponential and Weibull model in data simulated by lognormal distributions, which indicates that the treatment effect was not evaluated well. We further confirmed the deviated estimation through plots of survival probability versus time where Weibull and exponential model can not provide the prediction curve that match the actual. In this case, Cox model provided small MSEs, while means of $\beta$ are quite close to the truth. Also the prediction curve of Cox model was quite consistent with the actual. Thus we concluded that Cox model can estimate the treatment effect with high accuracy and precision which avoids the impacts of misspecifying the baseline hazard function. 

Case of the impact of fitting too complicated a model when an exponential is sufficient was found in simulation that $\gamma = 1$. In this case all three models had good performance in MSE and mean of $\beta$ with slight difference, which indicates that exponential model is sufficient. By observing the image, we noticed that the cox model had over-fitting to the data with low survival probability(the curve extended to T = 150), which may lead to a greater impact of end time data (such as censored data) in practical applications. Exponential prediction curve only extended to T = 50, which indicates that exponential model can avoid this impact since it has weaker dependence on end time data.

# Contributions

Charly Fowler worked on simulation functions, performed piecewise simulation, plotted baseline hazard curves, and created datasets of results.
Hanfei Qi worked on formatting simulation functions and editing plotting functions.
Robert Tumasian III worked on editing simulation functions, performing lognormal simulation, and creating datasets of results.
Haoyang Yi worked on creating plotting functions and editing simulation functions.
All members contributed equally to this project.

# References 

Andersen, Per Kragh, and Richard D. Gill. "Cox's regression model for counting processes: a large sample study." The annals of statistics (1982): 1100-1120.

Brilleman, Samuel, Rory Wolfe, Margarita Moreno-Betancur, & Michael J. Crowther. "Simulating Survival Data Using the simsurv R Package." \textit{Journal of Statistical Software} [Online], 97.3 (2021): 1 - 27. Web. 23 Feb. 2021